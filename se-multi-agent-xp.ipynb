{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5622bf-46a1-4272-b69e-eb23ce1a3474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18cef01-2f6e-4806-b11c-10027ac7e5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain-core==0.3.81 langchain-openai==0.3.35 langchain-community==0.3.31 langchain-text-splitters==0.3.11 langgraph==0.2.60 --break-system-packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d98cb5-4fc0-4386-bcb0-779b6fbdf451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "from typing import Annotated, Any, Generator, List, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_mcp import DatabricksMCPClient, DatabricksOAuthClientProvider\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from pydantic import create_model\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are a Schneider Electric Client Equipments Assistant.\n",
    "You have access to a Databricks Genie Room through a tool called query_space_01f0eca3dc1b1cbc9b65fa58f94c18ad\n",
    "This Tools can query data on your behalf and return responses\n",
    "It has access to 3 tables : \n",
    "  \n",
    "  1 - client_equipment_products\n",
    "  contains information about Schneider client equipment products, including details such as the client company, product category, equipment specifications, installation dates, and locations. This data can be used for tracking equipment inventory, managing installations, and analyzing product performance across different locations. It also includes additional resources like images and documentation links that may assist in product support.\n",
    "  Schema :\n",
    "    Company & Classification cols:\n",
    "      ENTREPRISE: Company name\n",
    "      CATEGORIE: Equipment category\n",
    "      type: Equipment type\n",
    "    Equipment Identification cols:\n",
    "      equipment_id: Unique equipment identifier\n",
    "      model: Model number\n",
    "      serial_number: Unique serial number\n",
    "      PRODUCT: Product name\n",
    "      product_ref: Product reference code [JOIN KEY ACROSS ALL TABLES]\n",
    "    Location cols:\n",
    "      location: Installation site\n",
    "      city: City location\n",
    "    Technical Specs cols:\n",
    "      voltage_rating: Voltage rating\n",
    "      installation_date: Installation date\n",
    "    Documentation cols:\n",
    "      title: Product title\n",
    "      description: Product details\n",
    "      NOTES: Additional notes\n",
    "      keyword: Search keywords\n",
    "      image: Image URL\n",
    "      downloadUrl: Document download link\n",
    "      openUrl: Additional resources link\n",
    "    Lifecycle cols:\n",
    "      endOfCommercializationDate: End of commercial availability date\n",
    "      \n",
    "  2 - client_equipments_telemetry\n",
    "  contains telemetry data related to Schneider client equipment. It includes information such as the equipment ID, product reference, model, and status, along with timestamps and any associated alarm codes or messages. This data can be used for monitoring equipment performance, troubleshooting issues, and managing inventory.\n",
    "  Schema :\n",
    "    Identification:\n",
    "      ENTREPRISE: Company name\n",
    "      equipment_id: Unique equipment identifier\n",
    "      product_ref: Product reference code [JOIN KEY ACROSS ALL TABLES]\n",
    "      model: Equipment model\n",
    "      type: Equipment type\n",
    "      CATEGORIE: Equipment category\n",
    "    Telemetry Data:\n",
    "      timestamp: Data recording date/time\n",
    "      status: Operational status\n",
    "      alarm_code: Alarm/error codes\n",
    "      message: Performance alerts and notifications\n",
    "\n",
    "  3 - mc.se.salesforce_cases\n",
    "  contains information related to Schneider customer support cases for various equipment. It includes details such as the enterprise involved, category of the case, model of the equipment, and product reference. The case history section provides a comprehensive view of each case, including status, customer details, resolution timelines, and technician notes. This data can be used to analyze support case trends, evaluate resolution efficiency, and identify common issues across different equipment models.\n",
    "  Schema :\n",
    "      equipment_id: Equipment identifier\n",
    "      entreprise: Company name\n",
    "      categorie: Industry sector\n",
    "      model: Equipment model\n",
    "      type: Equipment type\n",
    "      product_ref: Product reference number [THIS IS THE JOINING KEY BTW ALL TABLES]\n",
    "      case_history: Array of case records containing:\n",
    "        Case details (ID, status, customer)\n",
    "        Timeline (issue date, resolution date, days to resolve)\n",
    "        Technical info (equipment model, issue description, parts used, technician notes)\n",
    "        Resolution info (type, details, preventive actions)\n",
    "        Severity level\n",
    "\n",
    "Depending on client requests, you may formulate specific queries to the Genie Tool to retrieve relevant information from the tables. \n",
    "Use your reasoning skills to determine which calls to makes and how to articulate them ie :\n",
    "- Ask multiple targeted questions to the Genie space when needed\n",
    "- Use each answer to inform your next query\n",
    "- Build your reasoning chain step-by-step\n",
    "- Synthesize all gathered data into your final answer\n",
    "\n",
    "Think iteratively: query → analyze → refine → query again when needed until you have what you need to answer the user's question.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "host = workspace_client.config.host\n",
    "\n",
    "\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/genie/01f0eca3dc1b1cbc9b65fa58f94c18ad\", # Genie - Total Espark Catalog\n",
    "]\n",
    "\n",
    "# Custom MCP Servers — add URLs below (not managed or proxied by Databricks)\n",
    "CUSTOM_MCP_SERVER_URLS = [\n",
    "#    \"https://mcp-pmi-984752964297111.11.azure.databricksapps.com/mcp\"\n",
    "]\n",
    "\n",
    "#####################\n",
    "## MCP Tool Creation\n",
    "#####################\n",
    "\n",
    "# Define a custom LangChain tool that wraps functionality for calling MCP servers\n",
    "class MCPTool(BaseTool):\n",
    "    \"\"\"Custom LangChain tool that wraps MCP server functionality\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        args_schema: type,\n",
    "        server_url: str,\n",
    "        ws: WorkspaceClient,\n",
    "        is_custom: bool = False,\n",
    "    ):\n",
    "        # Initialize the tool\n",
    "        super().__init__(name=name, description=description, args_schema=args_schema)\n",
    "        # Store custom attributes: MCP server URL, Databricks workspace client, and whether the tool is for a custom server\n",
    "        object.__setattr__(self, \"server_url\", server_url)\n",
    "        object.__setattr__(self, \"workspace_client\", ws)\n",
    "        object.__setattr__(self, \"is_custom\", is_custom)\n",
    "\n",
    "    def _run(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the MCP tool\"\"\"\n",
    "        if self.is_custom:\n",
    "            # Use the async method for custom MCP servers (OAuth required)\n",
    "            return asyncio.run(self._run_custom_async(**kwargs))\n",
    "        else:\n",
    "            # Use managed MCP server via synchronous call\n",
    "            mcp_client = DatabricksMCPClient(\n",
    "                server_url=self.server_url, workspace_client=self.workspace_client\n",
    "            )\n",
    "            response = mcp_client.call_tool(self.name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "    async def _run_custom_async(self, **kwargs) -> str:\n",
    "        \"\"\"Execute custom MCP tool asynchronously\"\"\"\n",
    "        async with connect(\n",
    "            self.server_url, auth=DatabricksOAuthClientProvider(self.workspace_client)\n",
    "        ) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            # Create an async session with the server and call the tool\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.call_tool(self.name, kwargs)\n",
    "                return \"\".join([c.text for c in response.content])\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a custom MCP server (OAuth required)\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a custom MCP server using OAuth\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a managed MCP server\n",
    "def get_managed_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a managed MCP server\"\"\"\n",
    "    mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "    return mcp_client.list_tools()\n",
    "\n",
    "\n",
    "# Convert an MCP tool definition into a LangChain-compatible tool\n",
    "def create_langchain_tool_from_mcp(\n",
    "    mcp_tool, server_url: str, ws: WorkspaceClient, is_custom: bool = False\n",
    "):\n",
    "    \"\"\"Create a LangChain tool from an MCP tool definition\"\"\"\n",
    "    schema = mcp_tool.inputSchema.copy()\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "\n",
    "    # Map JSON schema types to Python types for input validation\n",
    "    TYPE_MAPPING = {\"integer\": int, \"number\": float, \"boolean\": bool}\n",
    "    field_definitions = {}\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type_str = field_info.get(\"type\", \"string\")\n",
    "        field_type = TYPE_MAPPING.get(field_type_str, str)\n",
    "\n",
    "        if field_name in required:\n",
    "            field_definitions[field_name] = (field_type, ...)\n",
    "        else:\n",
    "            field_definitions[field_name] = (field_type, None)\n",
    "\n",
    "    # Dynamically create a Pydantic schema for the tool's input arguments\n",
    "    args_schema = create_model(f\"{mcp_tool.name}Args\", **field_definitions)\n",
    "\n",
    "    # Return a configured MCPTool instance\n",
    "    return MCPTool(\n",
    "        name=mcp_tool.name,\n",
    "        description=mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "        args_schema=args_schema,\n",
    "        server_url=server_url,\n",
    "        ws=ws,\n",
    "        is_custom=is_custom,\n",
    "    )\n",
    "\n",
    "\n",
    "# Gather all tools from managed and custom MCP servers into a single list\n",
    "async def create_mcp_tools(\n",
    "    ws: WorkspaceClient, managed_server_urls: List[str] = None, custom_server_urls: List[str] = None\n",
    ") -> List[MCPTool]:\n",
    "    \"\"\"Create LangChain tools from both managed and custom MCP servers\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    if managed_server_urls:\n",
    "        # Load managed MCP tools\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_tools = get_managed_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=False)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        # Load custom MCP tools (async)\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=True)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "# Define the LangGraph agent that can call tools\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(\n",
    "        create_mcp_tools(\n",
    "            ws=workspace_client,\n",
    "            managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "            custom_server_urls=CUSTOM_MCP_SERVER_URLS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2417a321-d286-44c9-874f-4529041c70bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fba72bd-510f-4a94-8375-5a81cd944eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"List of Customers Having Equipment\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec2f8e4-ab7e-4cfe-9c11-b813c05b4d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b361a0-8fb3-46b3-8288-d436f294c8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import (\n",
    "  DatabricksVectorSearchIndex,\n",
    "  DatabricksServingEndpoint,\n",
    "  DatabricksSQLWarehouse,\n",
    "  DatabricksFunction,\n",
    "  DatabricksGenieSpace,\n",
    "  DatabricksTable,\n",
    "  DatabricksUCConnection,\n",
    "  DatabricksApp,\n",
    "  DatabricksLakebase\n",
    ")\n",
    "\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-claude-sonnet-4-5\"),\n",
    "    DatabricksSQLWarehouse(warehouse_id=\"148ccb90800933a1\"),\n",
    "    DatabricksGenieSpace(genie_space_id=\"01f0eca3dc1b1cbc9b65fa58f94c18ad\"),\n",
    "    DatabricksTable(table_name=\"mc.se.client_equipment_products\"),\n",
    "    DatabricksTable(table_name=\"mc.se.salesforce_cases\"),\n",
    "    DatabricksTable(table_name=\"mc.se.client_equipments_telemetry_final\"),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"databricks-mcp\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ],\n",
    "        input_example={\n",
    "            \"input\": [{\"role\": \"user\", \"content\": \"Example Request\"}]        \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34ce7ec-b462-4f57-91cf-a3cb2c086a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "\n",
    "UC_MODEL_NAME = \"mc.se.assistant\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bb06bf-37b2-49be-943d-694d7365efe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "se-multi-agent-xp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}